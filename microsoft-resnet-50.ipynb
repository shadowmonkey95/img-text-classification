{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pham_dinh_vu/code/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from transformers import AutoImageProcessor, ResNetForImageClassification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import joblib\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrademarkDataset(Dataset):\n",
    "    def __init__(self, df, target_column, img_dir, transform=None):\n",
    "        self.df = df\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_column = target_column\n",
    "        \n",
    "        # Encode target labels\n",
    "        self.label_encoder = LabelEncoder()\n",
    "        self.labels = self.label_encoder.fit_transform(df[target_column].str.split(',').str[0])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            img_path = os.path.join(self.img_dir, self.df.iloc[idx]['image_name'])\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "            return image, label\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {self.df.iloc[idx]['image_name']}: {str(e)}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_image_existence(df, img_dir):\n",
    "    \"\"\"\n",
    "    Check and filter the DataFrame to keep only rows where images exist and can be opened.\n",
    "    \"\"\"\n",
    "    missing_images = []\n",
    "    corrupted_images = []\n",
    "    valid_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Checking images\"):\n",
    "        img_path = os.path.join(img_dir, row['image_name'])\n",
    "        try:\n",
    "            # Try to open the image to check if it's valid\n",
    "            with Image.open(img_path) as img:\n",
    "                img.verify()  # Verify it's actually an image\n",
    "            valid_rows.append(True)\n",
    "        except (FileNotFoundError, Image.UnidentifiedImageError, IOError):\n",
    "            valid_rows.append(False)\n",
    "            if not os.path.exists(img_path):\n",
    "                missing_images.append(row['image_name'])\n",
    "            else:\n",
    "                corrupted_images.append(row['image_name'])\n",
    "    \n",
    "    valid_df = df[valid_rows].copy()\n",
    "    \n",
    "    print(f\"\\nTotal images in CSV: {len(df)}\")\n",
    "    print(f\"Missing images: {len(missing_images)}\")\n",
    "    print(f\"Corrupted images: {len(corrupted_images)}\")\n",
    "    print(f\"Valid images: {len(valid_df)}\")\n",
    "    \n",
    "    return valid_df, missing_images, corrupted_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_trademark_classifier(df, target_column, img_dir, num_epochs=10, batch_size=32):\n",
    "    train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
    "    \n",
    "    # Define transformations\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                           std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create datasets\n",
    "    train_dataset = TrademarkDataset(train_df, target_column, img_dir, transform)\n",
    "    val_dataset = TrademarkDataset(val_df, target_column, img_dir, transform)\n",
    "    test_dataset = TrademarkDataset(test_df, target_column, img_dir, transform)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(train_dataset.label_encoder.classes_)\n",
    "    model = ResNetForImageClassification.from_pretrained(\n",
    "        \"microsoft/resnet-50\",\n",
    "        num_labels=num_classes,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    \n",
    "    # Move model to GPU if available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Define optimizer and loss function\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_acc = 0\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images).logits\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images).logits\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'Train Loss: {train_loss/len(train_loader):.4f}, '\n",
    "              f'Train Acc: {100.*train_correct/train_total:.2f}%')\n",
    "        print(f'Val Loss: {val_loss/len(val_loader):.4f}, '\n",
    "              f'Val Acc: {100.*val_correct/val_total:.2f}%')\n",
    "        \n",
    "        # Save best model\n",
    "        val_acc = 100.*val_correct/val_total\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save(model.state_dict(), f'best_model_{target_column}.pth')\n",
    "    \n",
    "    # Test phase\n",
    "    model.load_state_dict(torch.load(f'best_model_{target_column}.pth'))\n",
    "    model.eval()\n",
    "    \n",
    "    test_predictions = []\n",
    "    test_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images).logits\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_predictions.extend(predicted.cpu().numpy())\n",
    "            test_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Convert numeric predictions back to original labels\n",
    "    pred_labels = train_dataset.label_encoder.inverse_transform(test_predictions)\n",
    "    true_labels = train_dataset.label_encoder.inverse_transform(test_labels)\n",
    "    \n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(true_labels, pred_labels))\n",
    "    \n",
    "    return model, train_dataset.label_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting trademark classification pipeline...\")\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv('./data/csv/pretrain_fill.csv')\n",
    "    img_dir = './data/img/'  # Updated path to match your structure\n",
    "    \n",
    "    # Validate images first\n",
    "    print(\"\\nValidating image files...\")\n",
    "    valid_df, missing_images, corrupted_images = validate_image_existence(df, img_dir)\n",
    "    \n",
    "    # Save validation results\n",
    "    if len(missing_images) > 0 or len(corrupted_images) > 0:\n",
    "        print(\"\\nWarning: Some images are missing or corrupted!\")\n",
    "        \n",
    "        # Save missing images list\n",
    "        with open('missing_images.txt', 'w') as f:\n",
    "            f.write(\"Missing images:\\n\")\n",
    "            for img in missing_images:\n",
    "                f.write(f\"{img}\\n\")\n",
    "            f.write(\"\\nCorrupted images:\\n\")\n",
    "            for img in corrupted_images:\n",
    "                f.write(f\"{img}\\n\")\n",
    "        print(\"Image issues list saved to 'missing_images.txt'\")\n",
    "        \n",
    "        # Save valid dataset\n",
    "        valid_df.to_csv('pretrain_fill_valid.csv', index=False)\n",
    "        print(\"Valid dataset saved to 'pretrain_fill_valid.csv'\")\n",
    "    \n",
    "    # Check if we have enough data to proceed\n",
    "    if len(valid_df) < 10:\n",
    "        print(\"Error: Not enough valid images to train. Please check your dataset.\")\n",
    "        return\n",
    "    \n",
    "    # Train models for different hierarchical levels\n",
    "    # hierarchical_levels = ['target', 'target_h1', 'target_h2', 'target_h3']\n",
    "    hierarchical_levels = ['target_h2']\n",
    "    \n",
    "    for level in hierarchical_levels:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Training model for {level}\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        try:\n",
    "            model, label_encoder = train_trademark_classifier(valid_df, level, img_dir)\n",
    "            \n",
    "            # Save label encoder for later use\n",
    "            joblib.dump(label_encoder, f'label_encoder_{level}.pkl')\n",
    "            print(f\"\\nModel and label encoder for {level} saved successfully\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\nError training model for {level}: {str(e)}\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting trademark classification pipeline...\n",
      "\n",
      "Validating image files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking images: 100%|██████████| 158511/158511 [01:31<00:00, 1729.60it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total images in CSV: 158511\n",
      "Missing images: 55178\n",
      "Corrupted images: 12\n",
      "Valid images: 103321\n",
      "\n",
      "Warning: Some images are missing or corrupted!\n",
      "Image issues list saved to 'missing_images.txt'\n",
      "Valid dataset saved to 'pretrain_fill_valid.csv'\n",
      "\n",
      "==================================================\n",
      "Training model for target_h2\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ResNetForImageClassification were not initialized from the model checkpoint at microsoft/resnet-50 and are newly initialized because the shapes did not match:\n",
      "- classifier.1.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([152]) in the model instantiated\n",
      "- classifier.1.weight: found shape torch.Size([1000, 2048]) in the checkpoint and torch.Size([152, 2048]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/10: 100%|██████████| 2261/2261 [17:11<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:\n",
      "Train Loss: 3.8960, Train Acc: 14.14%\n",
      "Val Loss: 4.9200, Val Acc: 3.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 2261/2261 [17:30<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10:\n",
      "Train Loss: 3.4234, Train Acc: 21.55%\n",
      "Val Loss: 4.8438, Val Acc: 4.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 2261/2261 [17:19<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10:\n",
      "Train Loss: 3.1842, Train Acc: 26.20%\n",
      "Val Loss: 4.8314, Val Acc: 7.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 2261/2261 [17:20<00:00,  2.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10:\n",
      "Train Loss: 2.9987, Train Acc: 30.12%\n",
      "Val Loss: 4.8141, Val Acc: 8.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 2261/2261 [17:14<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10:\n",
      "Train Loss: 2.8546, Train Acc: 33.03%\n",
      "Val Loss: 4.8120, Val Acc: 10.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 2261/2261 [17:12<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:\n",
      "Train Loss: 2.7364, Train Acc: 35.33%\n",
      "Val Loss: 4.8267, Val Acc: 11.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 2261/2261 [17:15<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10:\n",
      "Train Loss: 2.6357, Train Acc: 37.37%\n",
      "Val Loss: 4.8122, Val Acc: 12.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 2261/2261 [17:17<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10:\n",
      "Train Loss: 2.5449, Train Acc: 38.96%\n",
      "Val Loss: 4.7880, Val Acc: 12.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 2261/2261 [17:16<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10:\n",
      "Train Loss: 2.4616, Train Acc: 40.62%\n",
      "Val Loss: 4.8566, Val Acc: 13.48%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 2261/2261 [17:15<00:00,  2.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10:\n",
      "Train Loss: 2.3853, Train Acc: 42.15%\n",
      "Val Loss: 4.8869, Val Acc: 14.39%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       01.01       0.40      0.56      0.47       609\n",
      "       01.03       0.00      0.00      0.00        12\n",
      "       01.05       0.38      0.36      0.37       206\n",
      "       01.07       0.47      0.64      0.54       250\n",
      "       01.09       0.62      0.11      0.19        44\n",
      "       01.11       0.00      0.00      0.00        45\n",
      "       01.15       0.23      0.26      0.24       889\n",
      "       01.17       0.50      0.04      0.07        51\n",
      "       02.01       0.34      0.49      0.40       690\n",
      "       02.03       0.42      0.31      0.36       183\n",
      "       02.05       0.41      0.21      0.28        86\n",
      "       02.07       0.00      0.00      0.00         6\n",
      "       02.09       0.00      0.00      0.00        19\n",
      "       02.11       0.28      0.23      0.26       562\n",
      "       03.01       0.29      0.50      0.36       309\n",
      "       03.03       0.00      0.00      0.00        43\n",
      "       03.05       0.60      0.10      0.17        59\n",
      "       03.07       0.29      0.28      0.28       105\n",
      "       03.09       0.25      0.05      0.09        37\n",
      "       03.11       0.00      0.00      0.00        14\n",
      "       03.13       0.00      0.00      0.00        76\n",
      "       03.15       0.34      0.45      0.39       308\n",
      "       03.17       0.25      0.05      0.08        84\n",
      "       03.19       0.36      0.03      0.06       121\n",
      "       03.21       0.71      0.07      0.12        75\n",
      "       03.23       0.27      0.21      0.24       111\n",
      "       03.25       0.00      0.00      0.00         3\n",
      "       04.01       0.00      0.00      0.00        56\n",
      "       04.03       0.00      0.00      0.00         4\n",
      "       04.05       0.20      0.02      0.04        47\n",
      "       04.07       0.00      0.00      0.00         2\n",
      "       04.09       0.00      0.00      0.00         4\n",
      "       05.01       0.31      0.29      0.30       171\n",
      "       05.03       0.39      0.39      0.39       401\n",
      "       05.05       0.29      0.35      0.31       221\n",
      "       05.07       0.00      0.00      0.00        60\n",
      "       05.09       0.29      0.03      0.05        68\n",
      "       05.11       0.00      0.00      0.00        29\n",
      "       05.13       0.00      0.00      0.00        44\n",
      "       05.15       0.33      0.10      0.15        50\n",
      "       06.01       0.29      0.10      0.15        50\n",
      "       06.03       0.00      0.00      0.00        44\n",
      "       06.07       0.00      0.00      0.00        16\n",
      "       06.09       0.00      0.00      0.00        13\n",
      "       07.01       0.30      0.32      0.31        62\n",
      "       07.03       0.22      0.10      0.14        49\n",
      "       07.05       0.00      0.00      0.00         8\n",
      "       07.07       0.00      0.00      0.00        36\n",
      "       07.09       0.00      0.00      0.00        42\n",
      "       07.11       0.00      0.00      0.00        51\n",
      "       07.13       0.00      0.00      0.00        11\n",
      "       07.15       0.00      0.00      0.00         9\n",
      "       08.01       0.00      0.00      0.00        30\n",
      "       08.03       0.00      0.00      0.00        14\n",
      "       08.05       0.00      0.00      0.00         3\n",
      "       08.07       0.00      0.00      0.00         2\n",
      "       08.09       0.00      0.00      0.00        12\n",
      "       08.11       0.00      0.00      0.00         2\n",
      "       08.13       0.00      0.00      0.00         8\n",
      "       09.01       0.00      0.00      0.00        66\n",
      "       09.03       0.32      0.39      0.35        31\n",
      "       09.05       0.00      0.00      0.00        25\n",
      "       09.07       1.00      0.04      0.08        25\n",
      "       09.09       0.00      0.00      0.00        18\n",
      "       10.01       0.00      0.00      0.00         4\n",
      "       10.03       0.00      0.00      0.00        14\n",
      "       10.05       0.00      0.00      0.00         8\n",
      "       10.07       0.00      0.00      0.00         5\n",
      "       10.09       0.00      0.00      0.00         7\n",
      "       11.01       0.00      0.00      0.00        39\n",
      "       11.03       0.00      0.00      0.00        29\n",
      "       11.05       0.00      0.00      0.00         6\n",
      "       11.07       0.00      0.00      0.00         4\n",
      "       11.09       0.00      0.00      0.00        13\n",
      "       12.01       0.00      0.00      0.00         3\n",
      "       12.03       0.00      0.00      0.00        26\n",
      "       13.01       0.00      0.00      0.00         9\n",
      "       13.03       0.00      0.00      0.00        52\n",
      "       14.01       0.00      0.00      0.00        48\n",
      "       14.03       0.00      0.00      0.00        33\n",
      "       14.05       0.00      0.00      0.00         4\n",
      "       14.07       0.00      0.00      0.00         1\n",
      "       14.09       0.00      0.00      0.00        34\n",
      "       14.11       0.00      0.00      0.00        22\n",
      "       15.01       0.00      0.00      0.00         1\n",
      "       15.03       0.00      0.00      0.00        12\n",
      "       15.05       0.00      0.00      0.00        49\n",
      "       15.07       0.00      0.00      0.00        18\n",
      "       15.09       0.00      0.00      0.00        42\n",
      "       16.01       0.00      0.00      0.00        45\n",
      "       16.03       0.00      0.00      0.00        16\n",
      "       17.01       0.00      0.00      0.00        54\n",
      "       17.03       0.00      0.00      0.00         5\n",
      "       17.05       0.00      0.00      0.00        48\n",
      "       17.07       0.00      0.00      0.00        15\n",
      "       18.01       0.00      0.00      0.00        49\n",
      "       18.03       0.00      0.00      0.00        27\n",
      "       18.05       0.00      0.00      0.00        27\n",
      "       18.07       0.00      0.00      0.00        52\n",
      "       18.09       0.00      0.00      0.00         4\n",
      "       18.11       0.00      0.00      0.00         3\n",
      "       18.13       0.00      0.00      0.00        15\n",
      "       18.15       0.00      0.00      0.00        14\n",
      "       19.01       0.00      0.00      0.00        45\n",
      "       19.05       0.00      0.00      0.00        91\n",
      "       19.07       0.00      0.00      0.00         4\n",
      "       19.09       0.00      0.00      0.00         7\n",
      "       19.11       0.00      0.00      0.00        13\n",
      "       19.13       0.00      0.00      0.00        86\n",
      "       20.01       0.00      0.00      0.00        16\n",
      "       20.03       0.00      0.00      0.00        39\n",
      "       20.05       0.00      0.00      0.00        77\n",
      "       21.01       0.00      0.00      0.00        16\n",
      "       21.03       0.00      0.00      0.00         2\n",
      "       22.01       0.00      0.00      0.00         3\n",
      "       22.03       0.00      0.00      0.00        20\n",
      "       22.05       0.00      0.00      0.00        10\n",
      "       23.01       0.00      0.00      0.00         1\n",
      "       23.03       0.00      0.00      0.00       103\n",
      "       23.05       0.00      0.00      0.00         7\n",
      "       24.01       0.00      0.00      0.00         7\n",
      "       24.03       0.00      0.00      0.00        12\n",
      "       24.05       0.00      0.00      0.00        75\n",
      "       24.07       0.00      0.00      0.00        83\n",
      "       24.09       0.00      0.00      0.00        76\n",
      "       24.11       0.00      0.00      0.00       247\n",
      "       24.13       0.50      0.00      0.01       232\n",
      "       24.15       0.04      0.17      0.07        12\n",
      "       24.17       0.00      0.00      0.00        35\n",
      "       24.19       0.00      0.00      0.00        54\n",
      "       24.21       0.00      0.00      0.00      2019\n",
      "       25.01       0.00      0.00      0.00       451\n",
      "       25.03       0.00      0.00      0.00       403\n",
      "       26.01       0.00      0.03      0.00       160\n",
      "       26.03       0.01      0.00      0.00       524\n",
      "       26.05       0.03      0.01      0.02       886\n",
      "       26.07       0.01      0.01      0.01       190\n",
      "       26.09       0.02      0.08      0.03       127\n",
      "       26.11       0.06      0.07      0.06      1089\n",
      "       26.13       0.02      0.05      0.03        62\n",
      "       26.15       0.00      0.00      0.00         3\n",
      "       26.17       0.00      0.60      0.00         5\n",
      "       26.19       0.00      0.00      0.00         7\n",
      "       27.01       0.00      0.00      0.00       536\n",
      "       27.03       0.00      0.00      0.00        20\n",
      "       27.05       0.00      0.00      0.00         1\n",
      "       28.01       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.14     15499\n",
      "   macro avg       0.08      0.06      0.05     15499\n",
      "weighted avg       0.14      0.14      0.13     15499\n",
      "\n",
      "\n",
      "Model and label encoder for target_h2 saved successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pham_dinh_vu/code/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pham_dinh_vu/code/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pham_dinh_vu/code/.venv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
